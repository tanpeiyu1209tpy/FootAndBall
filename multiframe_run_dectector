# FootAndBall: Integrated Player and Ball Detector
# Jacek Komorowski, Grzegorz Kurzejamski, Grzegorz Sarwas
# Copyright (c) 2020 Sport Algorithmics and Gaming

#
# Run FootAndBall detector on ISSIA-CNR Soccer videos
#

import torch
import cv2
import os
import argparse
import tqdm
import json
from collections import deque
import numpy as np

import network.multiframe_footandball as footandball
from network.multiframe_footandball import MultiFrameFootAndBall
import data.augmentation as augmentations
from data.multiframe_augmentation import PLAYER_LABEL, BALL_LABEL
import metric  # Assumes metric.py is in same directory


def draw_bboxes(image, detections):
    font = cv2.FONT_HERSHEY_SIMPLEX
    for box, label, score in zip(detections['boxes'], detections['labels'], detections['scores']):
        if label == PLAYER_LABEL:
            x1, y1, x2, y2 = box
            color = (255, 0, 0)
            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
            cv2.putText(image, '{:0.2f}'.format(score), (int(x1), max(0, int(y1)-10)), font, 1, color, 2)

        elif label == BALL_LABEL:
            x1, y1, x2, y2 = box
            x = int((x1 + x2) / 2)
            y = int((y1 + y2) / 2)
            color = (0, 0, 255)
            radius = 25
            cv2.circle(image, (int(x), int(y)), radius, color, 2)
            cv2.putText(image, '{:0.2f}'.format(score), (max(0, int(x - radius)), max(0, (y - radius - 10))), font, 1, color, 2)
    return image

def run_detector_multiframe(model: footandball.MultiFrameFootAndBall, args: argparse.Namespace, num_frames=3, frame_interval=1):
    assert num_frames % 2 == 1, "num_frames must be odd (e.g. 3, 5)"
    half_len = num_frames // 2

    model.print_summary(show_architecture=False)
    model = model.to(args.device)

    print('Loading model weights...')
    state_dict = torch.load(args.weights, map_location=args.device)
    model.load_state_dict(state_dict)
    model.eval()

    sequence = cv2.VideoCapture(args.path)
    fps = sequence.get(cv2.CAP_PROP_FPS)
    frame_width = int(sequence.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(sequence.get(cv2.CAP_PROP_FRAME_HEIGHT))
    n_frames = int(sequence.get(cv2.CAP_PROP_FRAME_COUNT))
    out_sequence = cv2.VideoWriter(args.out_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))

    print('Processing video (multi-frame):', args.path)
    pbar = tqdm.tqdm(total=n_frames)

    frame_buffer = deque(maxlen=num_frames)
    raw_frames = []
    all_detections = []

    # Step 1: preload all frames
    while True:
        ret, frame = sequence.read()
        if not ret:
            break
        raw_frames.append(frame)
    sequence.release()

    # Step 2: loop over valid frame centers
    for i in range(half_len, len(raw_frames) - half_len):
        # Get temporal slice [iâˆ’1, i, i+1]
        indices = [i + j * frame_interval for j in range(-half_len, half_len + 1)]
        imgs = [raw_frames[idx] for idx in indices]

        # Apply transformation to each frame
        tensors = [augmentations.numpy2tensor(img) for img in imgs]
        input_tensor = torch.cat(tensors, dim=0).unsqueeze(0).to(args.device)  # Shape: [1, 3*num_frames, H, W]

        with torch.no_grad():
            detections = model(input_tensor)[0]

            frame_detections = {
                "boxes": [box.tolist() for box in detections["boxes"]],
                "scores": [score.item() for score in detections["scores"]],
                "labels": [label.item() for label in detections["labels"]]
            }
            all_detections.append(frame_detections)

        # Draw on center frame only
        center_frame = raw_frames[i].copy()
        center_frame = draw_bboxes(center_frame, detections)
        out_sequence.write(center_frame)

        pbar.update(1)

    pbar.close()
    out_sequence.release()

    return all_detections, half_len  # also return frame offset



if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--path', help='path to video', type=str, required=True)
    parser.add_argument('--model', help='model name', type=str, default='fb1')
    parser.add_argument('--weights', help='path to model weights', type=str, required=True)
    parser.add_argument('--ball_threshold', help='ball confidence detection threshold', type=float, default=0.5)
    parser.add_argument('--player_threshold', help='player confidence detection threshold', type=float, default=0.5)
    parser.add_argument('--out_video', help='path to output video', type=str, required=True)
    parser.add_argument('--device', help='device (CPU or CUDA)', type=str, default='cuda:0')
    parser.add_argument('--metric-path', help='Path to ground truth annotation (.xgtf) for evaluation', type=str)
    args = parser.parse_args()

    print('Video path: {}'.format(args.path))
    print('Model: {}'.format(args.model))
    print('Weights: {}'.format(args.weights))
    print('Output: {}'.format(args.out_video))
    print('Device: {}'.format(args.device))

    assert os.path.exists(args.weights), 'Weights not found'
    assert os.path.exists(args.path), 'Input video not found'

    model = footandball.multiframe_model_factory(args.model, 'detect',
                                      ball_threshold=args.ball_threshold,
                                      player_threshold=args.player_threshold)

    all_detections, offset = run_detector_multiframe(model, args, num_frames=3, frame_interval=1)

    # Load ground truth
    if args.metric_path:
        print("Loading ground truth from:", args.metric_path)
        gt_by_frame, gt_start_frame = metric.getGT(args.metric_path)
        print(f"Loaded {len(gt_by_frame)} frames of ground truth. Start from frame {gt_start_frame}")

        gt_by_frame = gt_by_frame[gt_start_frame + offset:]
        
        # Ensure detection and GT have same number of frames
        if len(all_detections) > len(gt_by_frame):
            all_detections = all_detections[:len(gt_by_frame)]
        elif len(gt_by_frame) > len(all_detections):
            gt_by_frame = gt_by_frame[:len(all_detections)]

        ap_results = metric.compute_ap_map(all_detections, gt_by_frame)

        print("\n===== Evaluation Results =====")
        print(f"Ball AP@0.5:   {ap_results.get(BALL_LABEL, 0.0):.4f}")
        print(f"Player AP@0.5: {ap_results.get(PLAYER_LABEL, 0.0):.4f}")
        print(f"mAP@0.5:       {ap_results.get('mAP', 0.0):.4f}")

        with open("ap_results.json", "w", encoding="utf-8") as f:
            json.dump({
                "ball_ap": ap_results.get(BALL_LABEL, 0.0),
                "player_ap": ap_results.get(PLAYER_LABEL, 0.0),
                "mAP@0.5": ap_results.get('mAP', 0.0)
            }, f, indent=2)
    else:
        print("No metric path provided. Skipping mAP evaluation.")

